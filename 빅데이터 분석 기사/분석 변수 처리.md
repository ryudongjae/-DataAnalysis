# 분석 변수 처리

### 변수의 개념

변수란 데이터를 담는 저장소로 데이터 베이스 관리시스템에서는 속성이라고 부른다.

- 독립변수
    - 다른 변수들에 의해 영향을 받지 않으며, 종속변수에 영향을 주는 변수
    - 설명변수, 예측변수 , 입력변수 , 조작변수라고도 함
- 종속변수
    - 독립변수에 의해 영향을 받는 변수
    - 반응변수 , 결과변수, 출력변수 라고도 함

**변수가 담고 있는 데이터의 유형**

- 범주형
    - 변수가 담고 있는 데이터가 몇 개의 범주로 나누어진 자료를 의미함
    - 명목형
        - 측정값이 일정한 범주에 속하도록 이름을 붙이지만 , 각 범주 간에 수누이가 없는 변수를 의미함
        - 성별 → 남/녀
    - 순서형
        - 측정값이 일정한 범주에 속하도록 이름을 붙이지만, 각 범주 간에 순위가 있는 변수를 의미함
        - 예) 성적 → 1등급,2등급…
- 수치형
    - 변수가 담고 있는 데이터가 수치형 자료로 포현되는 경우 이를  수치형 변수라고 함
    - 연속형
        - 측정값들이 서로 연속된 값을 갖는 경우 (키, 몸무게)
    - 이산형
        - 변수가 취할 수 있는 값들을 셀 수 있는 경우( 아파츠 층수 , 회사 직원수)

### 변수 선택

분석 모형에 가장 적절한 변수를 선택하는 과정을 변수 선택 혹은 피처 선택이라고 한다.

**변수 선택 방법**

- 필터 방법 (Filter Method)
    - 특정 모델링 기법에 의존하지 않고 데이터에 대한 통계적 특징을 이용해 변수를 선택하는 방법
    - 변수 간의 연관성 측정
    - 0에 가까운 분산 (Near Zero Variance)
        - 변수를 선택해 가장 단순한 방법은 0에 가까운 분산을 갖는 변수를 제거하는 것이다.
    - 큰 상관 계수 (Correlation Coefficient)
        - 두 변수 간 선형관계를 묘사하는 통계적 지표는 상관계수이다.
- 래퍼 방법 (Wrapper Method)
    - 변수의 일부만을 사용해 모델링을 수행하고 그 경과를 확인하는 작업을 반복하여 변수를 선택하는 방법
    - 최고의 피처 집합을 찾아가기 위해 모델링을 여러 번 수행해야 하므로 시간이 오래걸리며 과적합 위험이 있음
    - 회귀모델에서의 변수 선택 알고리즘
        - Forward Selection(전진 선택) , Backward Elimination(후진 제거) , Stepwise Selection(단계별 선택) , Best Subset

- 임베디드 방법 (Embedded Method)
    - 모델링 기법 자체에 변수 선택이 포함되어 있는 방법
    - 임베디드 방법
        - 라쏘 회귀
            - 가중치 절대값의 합을 최소화하는 것을 제약조건으로 추가하는 방법이다.
        - 릿지 회귀
            - 가중치들의 제곱합을 최소화하는 것을 제약조건으로 추가하는 기법이다.
        - 엘라스틱 넷
            - 라쏘와 릿지를 결합한 방식이다,

### 차원 축소

차원축소란 수많은 변수들로 구성된 다치원 데이터 셋에서 변수의 개수를 줄임으로서 차원을 축소하여 새로운 차원의 데이터를 생성되는것을 의미한다. 

분석에 사용할 데이터의 변수가 많을 때는 적덜한 차원축소 기법을 사용하여 변수를 축소하고 데이터를 잘 설명할 수 있는 변수들만 선택하여 분석에 활용할 수 있다.

**차원 축소 유형**

- 피처 선택 (Feature Selection)
    - 여러 변수들 가운데서 데이터의 특징을 가장 잘 나타내는 주요 피처만 선택하는 것
- 피처 추출 (Feature Extraction)
    - 기존 변수들 간의 관계를 파악하여 이들을 잘 표현할 수 있도록 선형 혹은 비선형 결합을 활용해 새로운 피처를 추출하는것

**차원 축소 방법** 

- 다차원 척도법 (MDS  → multi - Dimensional Scaling)
    - 데이터 속에 잠재해 있는 패턴, 구조를 찾아내어 소수 차원의 공간에 객체간 근접성을 시각화하는 통계 기법이다.
- 주성분 분석 (PCA → Principal Component Analysis)
    - 분석변수들을 선형적으로 결합하여 데이터를 가장 잘 표현할 수 있는 축을 찾아내고 그 축을 중심으로 데이터의 차원을 축소해서 표현해주는 기법이다.
- 선형 판별 분석 (LDA  → Linear Discriminant Analysis)
    - LDA는 PCA와 유사하게 데이터를 저차원 공간에 투영해 차원을 축소하는 방법이다.
- 요인분석 (Factor Analysis)
    - 여러 개의 변수들로 이루어진 데이터에서 변수들 간의 상관관계를 고려하여 서로 유사한 변수들을 묶어 새로운 잠재요인을 추출해내는 분석 방법이다.
- 특이값 분해 (SVD → Singular Value Decomposition)
    - 선형 대수의 일반적인 기법이며,  술수 공간에서 정의된 m * n 차원의 행렬에서 특이값을 추출하고 이를 활용해 데이터의 차원을 축소하는 방법이다.

위와 같은 기법들과 알고리즘을 사용하기 전에 반드시 각 분석변수들의 독립성을 확인해야 한다.

**차원 축소의 장점** 

- 차원축소를 수행하면 데이터의 양이 줄어 시간 복잡도와 공간 복잡도가 줄어든다.
- 변수들 간의 상관성을 고려하여 데이터의 차원을 줄여서 학습 모델이 간단해지고 안정적인 결과를 내놓을수 있다.

**차원 축소 활용**

- 모델링 시 주요 설명변수 추출
- 탐색족 데이터 분석
- 다차원 공간의 정보를 저차원으로 변환
- 데이터 내의 중요한 변수 혹은 잠재된 새로운 요인 발견

### 파생변수 생성

**파생변수**

- 사용자가 특정 조건을 만족하거나 특정 함수에 의햐 값을 만들어 의미를 부여한 변수이다.
- 매우 주관적일 수 있으므로 논라족 타당성을 갖추어 개발해야 한다.
- 세분화, 고객 행동 예측, 캠페인 반응 예측에 매우 잘 활용된다.

**요약변수**

- 수집된 정보를 분석에 맞게 종합한 변수이다.
- 많은 모델에서 공통으로 사용될 수 있어 재활용성이 높다.

### 변수 변환

**변수의 구간화** 

<aside>
💡 데이터에 개입된 노이즈를 제거하기 위해서는 연속형 변수를 다수의 구간으로 나눈고 동일한 구간에 속하는 변수 값들을 하나의 변수값으로 변환하는 구간화 기법이 사용될 수 있다.

</aside>

구간화 방법

- 변수 구간화
    - 데이터 분석의 성능을 향상시키기 위해서 혹은 해석의 편리성을 위해 이산형 변수를 범주형 변수로 변환하기도 하는데, 이를 변수 구간화라고 한다.
- 의사결정나무
    - 세분화 또는 예측에 활용되는 의사결정나무 모형을 사용하여 입력변수들을 구간화 할 수 있다.

**더미 변수**

- 범주형 변수 변환
    - 범주형 변수가 존재하는 데이터에 대해 회귀분석과 같이 연속형 변수를 다루는 기법을 적용하기 위해서는 범주형 변수를 연속형 변수로 변환하는 과정이 필요하다.
- R의 lm함수의 범주형 변수 처리 예시
    - R에서 회귀모형을 생성하는 lm함수는 데이터에 범주형 변수가 포함되어 있을 경우, 이를 자동으로 더미 변수로 변환하여 회귀모형을 생성한다.
- 정규분포화  (Normal Distribution)
    - 데이터가 정규분포를 따르지 않을 경우 변수 변환을 고려할 수 있다.
    - 변환 이전에 변수의 분포형태를 살펴서 정규분포를 따르고 있는지 판단하여 데이터 변환을 실시해야 하며 , 로그 변환 ,제곱근 변환, 역변환 들의 방법을 사용하여 데이터를 정규 분포를 따르는 형태로 변환된다.
    - 로그 변환 (log Transformation)
        - 데이터를 정규분포와 가깡운 형태로 변환하고자 할 때 사용하는 방법으로 변환하고자 하는 변수에 로그함수를 취하는 것이 로그 변환이다.
    - 재곱근 변환(Square Root Transformation)
        - 오른쪽으로 긴 꼬리를 갖는 분포의 데이터의 대칭화에 유용하며, 변환 하고자 하는 변수 값을 제곱근하여 변환한다.
    

### 불균형 데이터 처리

1. 데이터 불균형
    1. 데이터의 불균형은 분류할 각 집단에 속하는 데이터의 수가 동일하지 않은 경우를 의미한다.
2. 오버 샘플링(Over Sampling)
    1. 오버 샘플링은 더 작은 수의 데이터를 가지고 있는 집단을 표본으로 더 많이 추출하여 데이터 불균형을 해결하는 방법이다.
    2. 오버 샘플링은 데이터의 수를 증가시켜 계산에 필요한 시간이 커지거나 복제되는 데이터에 분류기가 과적응할수 있다는 단점이 있다.
    3. 오버 샘플링 종류
        1. Resampling
            1. 더 작은 수의 관측치를 가지고 있는 집단의 데이터를 복사하여 그 수를 늘리는 방법이다.
        2. SMOTE (Synthetic Minority Oversampling Technique)
            1. SMOTE는 기존에 있는 데이터를 복제하는 대신 더 작은 수의 관측치를 가지고 있는 집단의 데이터들과 일정한 거리를 가진 가장의 데이터들을 새로 생성하여 오버 샘플링을 진행하는 방법이다.
        3. Borderline SMOTE
            1. 서로 다른 클래스 간의 경계선에서 SMOTE를 적용하는 방법이다.
        4. ADASYN(Adaptive Synthetic Sampling Approach)
            1. Borderline SMOTE와 비슷하지만 데이터의 위치에 따라 샘플링 개수를 다르게 설정한다는 점이 다르다.
3. 언더 샘플림(Under Sampling)
    1. 언더 샘플링은 더 많은 수의 데이터를 가지고 있는 집단의 일부만 추출하여 데이터 샘플링을 진행하는 방법이다.
    2. 언더 샘플링을 통해 데이터 불균형을 해결할 경우, 데이터의 크기가 매우 클 때 효과적이지만 데이터의 일부를 버림으로써 중요한 데이터를 잃거나 정보가 손실된다는 단점이 있다.
    3. 언더 샘플링 종류
        1. 램덤 언더 샘플링(Random Under Sampling)
            1. 더 많은 수의 데이터를 가지고 있는 집단의 관측치 중 일부를 무작위로 샘플링하는 것이다. 랜덤 추출이므로 수행할 때마다 다른 결과가 도출된다. 
        2. Tomek Links
            1. 클래스를 구분하는 경계선 가까이에 존재하는 데이터, 즉 서로 다른 클래스에 속하지만 가장 가까이에 붙어있는 한 쌍의 데이터를 의미한다.
        3. CNN(Condensed Nearest Neighbor)
            1. 더 많은 수의 데이터를 가지고 있는 집단의 관측치들 중 밀집된 데이터를 제거하여 해당 집단을 대표할 수 있는 데이터만 남기고 샘플링을 진행하는 방법이다.
        4. OSS(One-side Selection)
            1. 토멕 링크과 CNN의 장점을 섞은 방법이다.